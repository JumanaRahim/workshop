{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S8kjLOxQ5tAGsUASrmLhMCq7ppilm1-U",
      "authorship_tag": "ABX9TyO9W5E4oaDrr15ANZUyCSIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JumanaRahim/workshop/blob/master/LND_CNN_KERAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri_-02a7cn31",
        "outputId": "bbc578b6-78fb-42b6-efa4-86108195952d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "!pip install imutils\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5leVea3c6mu",
        "outputId": "9abe7708-9ff4-4915-d469-883ba7ebe232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.8/dist-packages (0.5.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchsummary import summary\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from imutils import paths\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fhHY4nLgc_YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root='/content/drive/MyDrive/JSRT_/dataset/dataset'"
      ],
      "metadata": {
        "id": "u3nV00LYdGUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=('benign', 'malignant', 'non-nodule')\n",
        "nb_classes = len(classes)\n",
        "image_size = (150,150)"
      ],
      "metadata": {
        "id": "h4xS0Bdud5ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = list(paths.list_images('/content/drive/MyDrive/JSRT_/dataset/dataset'))\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "for img_path in image_paths:\n",
        "    label = img_path.split(os.path.sep)[-2]\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    images.append(img)\n",
        "    labels.append(label)\n",
        "    \n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wfK0Bq9Dd8A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        " images, labels = load_data('/content/drive/MyDrive/JSRT_/dataset/dataset')"
      ],
      "metadata": {
        "id": "1Irw0ZA1nDLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uzCUoxmechJ",
        "outputId": "be275694-4c95-442e-8f01-bcd99493379f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'non-nodule', 'non-nodule', 'non-nodule',\n",
              "       'non-nodule', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
              "       'malignant', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign', 'benign', 'benign', 'benign', 'benign', 'benign',\n",
              "       'benign'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = LabelEncoder()\n",
        "labels = lb.fit_transform(labels)\n",
        "print(f\"Total Number of Classes: {len(lb.classes_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1FlldJzejaf",
        "outputId": "528ce05a-2129-4174-cf9c-d397546d5159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Classes: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#orginal dataset\n",
        "\n",
        "train_orginal_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_orginal_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.ToTensor(),\n",
        "])  \n",
        "#for data augmentation\n",
        "\n",
        "train_augmentation_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(p=0.6),\n",
        "    transforms.RandomVerticalFlip(p=0.6),\n",
        "    transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "test_augmentation_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(p=0.6),\n",
        "    transforms.RandomVerticalFlip(p=0.6),\n",
        "    transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "Ds9mMYxSpB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.2)\n",
        "test_images, val_images, test_labels, val_labels = train_test_split(test_images, test_labels, test_size = 0.5)\n",
        "print(f\"train_images examples: {train_images.shape}\\ntest_images examples: {test_images.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxhrAWg1nnAD",
        "outputId": "3ddf8a74-dac2-4a1c-bd56-7fb543ff62b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images examples: (197, 2048, 2048, 3)\n",
            "test_images examples: (25, 2048, 2048, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transforms = None):\n",
        "        self.labels = labels\n",
        "        self.images = images\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        images= self.images[index][:]\n",
        "        labels = self.labels[index]\n",
        "        \n",
        "        if self.transforms:\n",
        "            images = self.transforms(images)\n",
        "            \n",
        "            return images,labels"
      ],
      "metadata": {
        "id": "QzxgoaBDe8Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        \n",
        "train_orginal_data = CustomDataset(train_images, train_orginal_transforms)\n",
        "test_orginal_data = CustomDataset(test_images, test_orginal_transforms)     \n",
        "\n",
        "\n",
        "train_augmentation_data = CustomDataset(train_images, train_augmentation_transforms )\n",
        "test_augmentation_data = CustomDataset(test_images, test_augmentation_transforms )  \n",
        "\n",
        "train_data = torch.utils.data.ConcatDataset([train_orginal_data,train_augmentation_data])\n",
        "print(len(train_data))\n",
        "test_data = torch.utils.data.ConcatDataset([test_orginal_data,test_augmentation_data])\n",
        "print(len(test_data))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainLoader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
        "testLoader = DataLoader(test_data, batch_size=32, shuffle=True, num_workers=4) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uRrLgF_fHsi",
        "outputId": "b0abdda6-a551-409f-cb35-cb55d308aae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "394\n",
            "50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = train_labels.shape[0]\n",
        "n_val = val_labels.shape[0]\n",
        "n_test = test_labels.shape[0]"
      ],
      "metadata": {
        "id": "b0Jv7Jh00nR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "metadata": {
        "id": "ipTbm4VXhX6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build CNN model\n",
        "#intialize\n",
        "cnn=tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "YspwmtKKhGIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten"
      ],
      "metadata": {
        "id": "voKwdcBShjFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convolution\n",
        "from keras.regularizers import l2\n",
        "cnn=tf.keras.models.Sequential()\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[2048,2048,3]))\n",
        "                              \n",
        "#Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
        "cnn.add(tf.keras.layers.Dropout(0.25))\n",
        "#add one more layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,activation='relu'\n",
        "      ))\n",
        "cnn.add(tf.keras.layers.Dropout(0.25))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=128,kernel_size=3,activation='relu'\n",
        "      ))\n",
        "cnn.add(tf.keras.layers.Dropout(0.25))\n",
        "#Flatten\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "#Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dropout(0.2))\n",
        "cnn.add(tf.keras.layers.Dense(units=64,activation='relu'))\n",
        "#Output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "7k9-5r_f3vPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model\n",
        "cnn.compile(optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "I5tW0Rhuhvb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpGzVOUh8b7",
        "outputId": "5b3dc023-b21b-4629-cfe5-7cb3101a2a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 74, 74, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 74, 74, 32)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 72, 72, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 34, 34, 128)       0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 147968)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               18940032  \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,041,601\n",
            "Trainable params: 19,041,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, verbose = 1, mode='min', restore_best_weights = True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(\n",
        "    monitor = 'val_accuracy', \n",
        "    patience = 2, \n",
        "    verbose = 1, \n",
        "    factor = 0.3, \n",
        "    min_lr = 0.000001)"
      ],
      "metadata": {
        "id": "x5CDsNlAifM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "test_data = np.array(test_images)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "EwfN9Jtlw5JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = cnn.fit(\n",
        "    train_data, train_labels,validation_data=(val_images, val_labels),\n",
        "    batch_size = 32, \n",
        "    epochs = 10,  \n",
        "    callbacks=[learning_rate_reduction])"
      ],
      "metadata": {
        "id": "nJtUwzyGh_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_loss_chart(history):\n",
        "    epochs = [i for i in range(10)]\n",
        "    fig , ax = plt.subplots(1,2)\n",
        "    train_acc = history.history['accuracy']\n",
        "    train_loss = history.history['loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "    fig.set_size_inches(20,10)\n",
        "    ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
        "    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n",
        "    ax[0].set_title('Training & Validation Accuracy')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
        "    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n",
        "    ax[1].set_title('Training & Validation Loss')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].set_ylabel(\"Training & Validation Loss\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9PyjMkj_IsH4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}